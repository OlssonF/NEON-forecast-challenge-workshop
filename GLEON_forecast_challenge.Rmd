---
title: "NEON forecast challenge - GLEON2022"
output:
  html_document: 
    number_sections: true
  pdf_document: 
    number_sections: true
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to NEON forecast challenge
The EFI RCN NEON Forecast Challenge asks the scientific community to produce ecological forecasts of future conditions at NEON sites by leveraging NEON's open data products. The Challenge is split into five themes that span aquatic and terrestrial systems, and population, community, and ecosystem processes across a broad range of ecoregions.

We are excited to use this Challenge to learn more about the predictability of ecological processes by forecasting NEON data prior to its release. Which modeling frameworks, mechanistic processes, and statistical approaches best capture community, population, and ecosystem dynamics? These questions are answerable by a community generating a diverse array of forecasts. The Challenge is open to any individual or team from anywhere around the world that wants to submit forecasts. Sign up [here](https://projects.ecoforecast.org/neon4cast-docs/Participation.html). 

### Aquatics challenge
What: Freshwater surface water temperature, oxygen, and chlorophyll-a.

Where: 7 lakes and 27 river/stream NEON sites.

When: Daily forecasts for at least 30-days in the future. New forecast submissions, that use new data to update the forecast, are accepted daily. The only requirement is that submissions are predictions of the future at the time the forecast is submitted.

Today we will focus on Lake sites only, following on from the MacrosystemEDDIE module, and will primarily start with thinking about water temperature. For the challenge, you can chose to submit to either the lakes, rivers or streams or all three! You can also chose to submit any of the three focal variables (temperature, oxygen, and chlorophyll). Find more information about the aquatics challenge [here](https://projects.ecoforecast.org/neon4cast-docs/Aquatics.html)

## Submission requirements
There are two key options for the format of the forcasts submitted to the Challenge. First, the file can be either in a _csv_ or a _netcdf_ format. Second the forecast must include a quantified uncertainty and the file can represent uncertainty using an ensemble forecast (multiple realisations of future conditions) or a distribution forecast (with mean forecast and standard deviation), specified in the family and parameter columns of the forecast file. 

For an ensemble forecast, the `family` column uses the word `ensemble` to designate that it is a ensemble forecast and the parameter column is the ensemble member number (1, 2, 3 …).  For a distribution forecast, the `family` column uses the word `normal` to designate a normal distribution and the parameter column must have values of mu and sigma for each forecasted variable, site_id, and datetime. For forecasts that don't have a normal distribution we recommend using the ensemble format and sampling from your non-normal distribution to generate a set of ensemble members that represents your distribution. I will go through examples of both `ensemble` and `normal` forecasts as examples. 

The full list of required columns and format can be found in the [Challenge documentation](https://projects.ecoforecast.org/neon4cast-docs/Submission-Instructions.html)

### Forecasting with fable {.unnumbered}

Two of these forecasting approaches will implement methods from the `fable` R package which is installed via `fpp3` package. The Fable package implements a range of different forecasting methods including Persistence Models, Moving Average Models, ARIMA Models and Time Series Models. The package integrates with `tidyverse` syntax and has good documentation and examples found [here](https://otexts.com/fpp3/).
`fable` and `fabletools`, are installed as part of the `fpp3` package and produce and deal with `mable` (model table) and `fable` (forecast table) objects. We will also use the `tidyverse` to manipulate and visualise the target data and forecasts. 

```{r, 'load packages', eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
library(fpp3)      # install.packages('fpp3'), package for forecasting
library(tsibble)   # install.packages('tsibble'), package for dealing with time series data sets
library(tidyverse) # install.packages('tidyverse'), collection of R packages for data manipulation, analysis and visualisation
library(neon4cast) # install via devtools::install_github('eco4cast/neon4cast)

# suppreses dplyr's summarise message
options(dplyr.summarise.inform = FALSE)
```

## 1. Read in the data
For all the forecasts we are creatin we need to start with looking at what the historic data says - this this the 'Targets'. This data is near real-time and is being uploaded to Cloud-based storage daily. We will start by reading in the data from the targets file. The data are updated regularly, with the latency for the aquatics data approximately 24-48 hrs. 
Information on the sites can be found in the `NEON_Field_Site_Metadata_20220412.csv`. We filter this to look at just the aquatic sites. This table has loads of information about the field sites, including location, ecoregion, information about the watershed (e.g. elevation, mean annual precipitation and temperature), and lake depth. 
 
```{r eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
#read in the targets data
targets <- read_csv('https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz')

# read in the sites data
aquatic_sites <- read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") |>
  dplyr::filter(aquatics == 1)
```

```{r eval = T, echo = F}
targets[1000:1010,]
```

The columns of the targets file show the time step (daily for aquatics challenge), the 4 character site code (`site_id`), the variable being measured, and the mean daily observation. To look at just the Lakes we can subset the targets and aquatic sites to those which have the `field_site_subtye` of `Lake`. 

```{r}
lake_sites <- aquatic_sites %>%
  filter(field_site_subtype == 'Lake')

targets <- targets %>%
  filter(site_id %in% lake_sites$field_site_id)
```

## 2. Visualise the data
```{r eval = T, echo = F, warning=FALSE, fig.dim=c(10,10), fig.cap=c('Figure: Temperature targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Oxygen targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Chlorophyll targets data at aquatic sites provided by EFI for the NEON forecasting challgenge. Chlorophyll data is only available at lake and river sites')}
targets %>%
  filter(variable == 'temperature') %>%
  ggplot(., aes(x = datetime, y = observation)) +
  geom_point() +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y') +
  labs(title = 'temperature')

targets %>%
  filter(variable == 'oxygen') %>%
  ggplot(., aes(x = datetime, y = observation)) +
  geom_point() +  
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')+
  labs(title = 'oxygen')

targets %>%
  filter(variable == 'chla') %>%
  ggplot(., aes(x = datetime, y = observation)) +
  geom_point() +   
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')+
  labs(title = 'chla')

```
We can think about what type of models might be useful to predict these variables at these sites. 

* We could use information about current conditions to predict the next day. What is happening today is usually a good predictor of what will happen tomorrow (Model 1- Persistence). 
* We could think about what the historic data tells us about this time of year. November this year is likely to be similar to November last year (Model 2 - Climatology/Seasonal Naive)
* And we could also look at the lake variables' relationship(s) with other variable. Could we use existing forecasts about the weather to generate forecasts about lake variables (Model 3 - Linear Model with Co-variates).


# Model 1: Persistence
`fable` has some simple models that can be fitted to the target data. Fable models require data to be in a tidy `tsibble` format. Tools for dealing with tsibble objects are found in the `tsibble` package. `tsibble` objects are similar in structure to `tibble` objects but have a built in timeseries element. This is specified in their creation as the  `index` or time variable. You also need to give the tsibble a `key`, which in combination with the index will uniquely identify each record. In our case the key variables will be `site_id` and `variable`. 

For RW forecasts, we simply set the forecast value be the value of the last observation. This is fine if we have data observed to yesterday but this often isn't the case. To make sure that the forecast uncertainty is reasonable, given the time since an observation, we start the forecast from the last observations and run forward 30 days into the future. Due to differences in past observations, the forecast will start from different dates depending on the date of the last observation at each site.

We calculate the `reference_datetime` and total `horizon` for each `site_id` and `variable` combination. These can then be subsetted when we run the random walk. 
```{r message=F}
# For the RW model need to start the forecast at the last non-NA day and then run to 30 days in the future
forecast_starts <- targets %>%
  filter(!is.na(observation)) %>%
  group_by(site_id, variable) %>%
  # Start the day after the most recent non-NA value
  summarise(reference_datetime = max(datetime) + 1) %>% # Date 
  mutate(h = (Sys.Date() - reference_datetime) + 30) %>% # Horizon value 
  ungroup() 

forecast_starts
```

You can see that the forecasts all have different start dates, based on when the last observation was taken. We want to fit each site and variable model separately depending on its start date and calculated horizon. To do this I have written a custom function that runs the RW forecast. Within this function we:

* Tidy: Takes the targets and fills with NAs, and filters up to the last non-NA value. The data must have explicit gaps for the full time series and must be in a tsibble format to run `fable`. Every time step up to the start of the forecast must exist even if it is filled with NAs (except the day before the forecast starts)!
* Specify: Fits the RW model. We can also specify transformations to use within the model.  The `fable` package will automatically back-transform common transformations in the forecasts whenever one is used in the model definition. Common transofrmations include box-cox, logarithmic, sqrt... The simplest specification of the persisten model is 
`RW_model <- targets_use %>% model(RW = RW(observation))` which stores the model table or `mable` in `RW_model`.
* Forecast: Then using this model, we run a forecast! We can specify whether bootstrapping is used and the number of bootstraps (`bootstrap = T`). Bootstrapping is where we run the forecast multiple times with a random error term drawn from the residuals of the fitted model. Doing this repeatedly, we obtain many possible futures. We decide how many of these possibilities to simulate with the `times=...` argument. The code to forecast using the specified model and bootstrapping is: 
`forecast <- RW_model %>% forecast(h = 30, bootstrap = T)`

Within this function, there are also if statements to test whether there are whole datasets missing etc. as well as messages which can be turned on/off with the `verbose = ` argument. 


```{r warning=FALSE, message =FALSE}
# Function carry out a random walk forecast
RW_daily_forecast <- function(site, 
                              var,
                              h,
                              bootstrap = FALSE, boot_number = 200,
                              transformation = 'none', verbose = TRUE,...) {
  
  # message('starting ',site_var_combinations$site[i], ' ', site_var_combinations$var[i], ' forecast')
  
  # filter the targets data set to the site_var pair
  targets_use <- targets %>%
    dplyr::filter(site_id == site,
           variable == var) %>%
    tsibble::as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
    
    # add NA values up to today (index)
    tsibble::fill_gaps(.end = Sys.Date()) %>%
    # Remove the NA's put at the end, so that the forecast starts from the last day with an observation,
    dplyr::filter(datetime < forecast_starts$reference_datetime[which(forecast_starts$site_id == site &
                                                                forecast_starts$variable == var)]) %>%
    mutate(observation = ifelse(observation == 0 & variable == "chla", 0.00001, observation))
  
  # Check to see if there are rows in the targets
  if (nrow(targets_use) == 0) {
    
    message('no targets available, no forecast run')
    
    empty_df <- data.frame('variable' = character(),
                            'site_id' = character(),
                            '.model' = character(),
                             'datetime' = lubridate::ymd(),
                            '.rep' = character(),
                            '.sim' = numeric())
    
    return(empty_df)
    
  } else {
    # Do you want to apply a transformation? 
    if (transformation == 'log') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(log(observation)))
    }
    
    if (transformation == 'log1p') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(log1p(observation)))
    }
    
    if (transformation == 'sqrt') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(sqrt(observation)))
    }
    
    if (transformation == 'none') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(observation))
    }
    
    # Do you want to do a bootstrapped forecast?
    if (bootstrap == T) {
      forecast <- RW_model %>% 
        fabletools::generate(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                                      forecast_starts$variable == var)]),
                             bootstrap = T,
                             times = boot_number)
    }  else
      forecast <- RW_model %>% 
        fabletools::forecast(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                                      forecast_starts$variable == var)]))
    
  if (verbose == T) {
    message(
      site,
      ' ',
      var,
      ' forecast with transformation = ',
      transformation,
      ' and bootstrap = ',
      bootstrap
    )
  }
    return(forecast)
    
  }
  
}

```

This function takes just one site and one variable as arguments. To run across all site_id-variable combinations we can use a `for` loop. We need a data frame that we can index from.  
The number of bootstraps (`boot_number`) is set to 200 and we say that we want to apply a `log()` transformation on the oxygen and chlorophyll values. 

We can then loop through each variable and site (row) and combine them into a single data frame (`RW_forecasts`).
```{r message = F}
site_var_combinations <- expand.grid(var = unique(targets$variable),
                                     site = unique(targets$site_id)) %>%
  # assign the transformation depending on the variable. chla and oxygen get a log(x ) transformation
  mutate(transformation = ifelse(var %in% c('chla', 'oxygen'), 
                                 'log', 
                                 'none')) 
head(site_var_combinations)
```


```{r }
# An empty data frame to put the forecasts in to
RW_forecast <- NULL

# Loop through each row (variable-site combination)
for (i in 1:nrow(site_var_combinations)) {
  
  forecast <- RW_daily_forecast(site = site_var_combinations$site[i],
                                var = site_var_combinations$var[i],
                                boot_number = 200,
                                bootstrap = T,
                                h = 30, 
                                verbose = F,
                                transformation = site_var_combinations$transformation[i])
  
  
  RW_forecast <- bind_rows(RW_forecast, forecast)
  
}

```

The output from the `forecast()` function is a forecast table or `fable`, which has columns for `variable`, `site_id`, the `.model`, the bootstrap value (1 to 200, `.rep`), and the prediction (`.sim`). 
```{r}
RW_forecast %>%
  filter(site_id == 'SUGG')
```

How reasonable are these forecasts?? Is there a way to improve the persistence model? Would another transformation work better?
Looking just at Suggs Lake the forecasts seem okay - but there's very high uncertainty
```{r, message = F, warning = F, echo = F, fig.cap = "Figure: Example 'random walk' forecasts for Lake Suggs (FL)"}
RW_forecast %>% 
  filter(site_id == 'SUGG') %>%
  ggplot(.,aes(x=datetime, y=.sim, group = .rep)) + geom_line(alpha = 0.4) + 
  geom_point(data = subset(targets, site_id == 'SUGG'),
            aes(x=datetime, y=observation, group = 'obs'), colour = 'black') +
  facet_wrap(~variable, scales = 'free') + 
  theme_bw() + theme(legend.position = 'none') +
  coord_cartesian(xlim = c(min(forecast_starts$reference_datetime[which(forecast_starts$site_id == 'SUGG')]) - 10,
                           Sys.Date() + 30)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed')
```
For Prairie Lake it's another question... 100 mg/L of dissolved oxygen??, -20 water temperature??

```{r, message = F, warning = F, echo = F, fig.cap = "Figure: Example 'random walk' forecasts for Prarie Lake"}
RW_forecast %>% 
  filter(site_id == 'PRLA') %>%
  ggplot(.,aes(x=datetime, y=.sim, group = .rep)) + geom_line(alpha = 0.4) + 
  geom_point(data = subset(targets, site_id == 'PRLA'),
            aes(x=datetime, y=observation, group = 'obs'), colour = 'black') +
  facet_wrap(~variable, scales = 'free') + 
  theme_bw() + theme(legend.position = 'none') +
  coord_cartesian(xlim = c(min(forecast_starts$reference_datetime[which(forecast_starts$site_id == 'SUGG')]) - 10,
                           Sys.Date() + 30)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed')
```

Each line on the plot is one of the ensemble members (shown in the fable as `.rep`). You can also see that not all the "forecasted" days are true forecasts (some are in the past), but we started the forecast at the last observation. Therefore when we write out the forecast and submit it we need to make sure to only submit the true forecast (of the future). 

## Convert to EFI standard for submission
For an ensemble forecast the documentation specifies the following columns:

* `datetime`: forecast timestamp
* `reference_datetime`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of reference_datetime in the file
* `site_id`: NEON code for site
* `family`: name of probability distribution that is described by the parameter values in the parameter column; only normal or ensemble are currently allowed.
* `parameter`: integer value for forecast replicate (i.e. `.rep`);
* `variable`: standardized variable name from the theme 
* `prediction`: forecasted value (from `.sim`)
* `model_id`: model name (no spaces)

The challenge also requires 'true' forecasts only (i.e predictions of future observations) so we filter for times in the future. 
```{r}
RW_forecasts_EFI <- RW_forecast %>%
  rename(parameter = .rep,
         prediction = .sim) %>%
  # For the EFI challenge we only want the forecast for future
  filter(datetime > Sys.Date()) %>%
  group_by(site_id, variable) %>%
  mutate(reference_datetime = min(datetime) - lubridate::days(1),
         family = 'ensemble',
         model_id = 'GLEON_RW') %>%
  select(datetime, reference_datetime, site_id, family, parameter, variable, prediction, model_id) 
```

Now we have a forecast that can be submitted to the EFI challenge. For example:

```{r}
RW_forecasts_EFI %>%
  filter(site_id == 'SUGG')
```

## Write the forecast to file
```{r eval = T}
# Start by writing the forecast to file
theme <- 'aquatics'
date <- Sys.Date()
forecast_name <- 'GLEON_RW_forecast.csv'
forecast_file <- paste(theme, date, forecast_name, sep = '-')

write_csv(RW_forecasts_EFI, forecast_file)

```
## Submit forecast 
Files need to be in the correct format with metadata for submission. The forecast organisers have created tools to help aid in the submission process. These tools can be downloaded from Github using `devtools::install_github(eco4cast/neon4cast)`.
These include functions for submitting, scoring and reading forecasts:

* `submit()` - submit the forecast file to the neon4cast server where it will be scored
* `forecast_output_validator()` - will check the file is in the correct format to be submitted
* `check_submission()` - check that your submission has been uploaded to the server
* `score`

```{r eval = FALSE}
# can uses the neon4cast::forecast_output_validator() to check the forecast is in the right format
neon4cast::submit(forecast_file = forecast_file,
                  ask = F) # if ask = T (default), it will produce a pop-up box asking if you want to submit
```

## TASKS
Possible modifications to the Model 1 (lines x-x):

* how does the number of ensembles change the uncertainty?
* are the transformations reasonable? Have a look [here](https://otexts.com/fpp3/ftransformations.html) about applying other types of transformations in Fable
* would expect there to be any long term trends in that are not captured? Try adding a drift() term to the model






# Model 2: Seasonal naive model
An alternative approach might be to look at the historic data to make predictions about the future. The seasonal naive model in `fable` sets each forecast to be equal to the last observed value given the specified lag. When we specify the lag to by 1 year, it will provide the forecast as the observations from the same day the previous year. 
Again we need to tidy the data to the correct format for `fable`. We make sure there are explicit gaps (using `fill_gaps()`) and make it into a tsibble with `variable` and `site_id` as the keys and `datetime` as the index. 
Then the `SNAIVE` model is fit with a 1 year lag.  One useful thing that the fable package can do is that it fits the specified models to each `key` pairing (variable, site_id) so you don't have to specify each model seperately. 

```{r, warning = F}
SN_model <- targets %>%
  as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
  # add NA values up to today (index)
  fill_gaps(.end = Sys.Date()) %>%
  
  # Here we fit the model
  model(SN = SNAIVE(observation ~ lag('1 year')))
```

Then we use the model we've specified to forecast. `h = 30` specifies the horizon of the forecast, relative to the index of the data (as 30 days). If the index, in this case `datetime`, had a different value such as monthly, the `h = ` value would be months. We use `forecast(... , bootstrap = F)` to run a non-bootstrapped forecast. The forecast will run for each key combination (variable-site_id). When bootstrap = F, the model assumes a normal distribution.

```{r, warning=FALSE, error=FALSE, message=FALSE}
SN_forecast <- SN_model %>% forecast(h = 30, bootstrap = F)
SN_forecast
```
The output from this function is a `fable`. The prediction are held in the `observation` column as an S3 distribution, which gives the mean and variance of the prediction. `N()` says that the distribution is normal, and gives the mean and variance. 

The challenge requires mean (mu) and standard deviation (sigma). We can calculate the standard deviation of the predicted values using a function to extract the variance and mean and create a table in the right format for EFI. The sigma represents the uncertainty in the forecast and is calculated from the residuals of the fitted model.

The columns needed for a distributional forecast are:

* `datetime`: forecast timestamp
* `reference_datetime`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of reference_datetime in the file
* `site_id`: NEON code for site
* `family`: name of probability distribution that is described by the parameter values in the parameter column; only normal or ensemble is currently allowed
* `parameter`: required to be the string mu (mean) or sigma (standard deviation) or the ensemble number
* `variable`: standardized variable name from the theme
* `prediction`: forecasted value for parameter listed in the parameter column (sigma or mu)

```{r}
convert.to.efi_standard <- function(df, model_id){
  ## determine variable name
  var <- attributes(df)$dist
  ## Normal distribution: use distribution mean and variance
  df %>% 
    dplyr::mutate(sigma = sqrt( distributional::variance( .data[[var]] ) ) ) %>%
    dplyr::rename(mu = .mean) %>%
    dplyr::select(datetime, site_id, .model, mu, sigma) %>%
    tidyr::pivot_longer(c(mu, sigma), names_to = "parameter", values_to = var) %>%
    dplyr::rename('prediction' = var) %>%
    mutate(family = "normal",
           reference_datetime = min(datetime) - lubridate::days(1),
           model_id = model_id) %>%
    select(any_of(c('model_id', 'datetime', 'reference_datetime', 'site_id', 'family', 'parameter', 'variable', 'prediction')))
}

```

```{r}
SN_forecast_EFI <- convert.to.efi_standard(SN_forecast, 
                                           model_id = 'GLEON_SN')
```
```{r echo = FALSE}
SN_forecast_EFI %>%
  filter(site_id == 'SUGG')
```

This is now in the right format to be submitted to the Challenge. 

```{r, message = F, warning = F, echo = F, fig.cap="Figure: Example 'seasonal naive' forecasts for Lake Suggs (FL). Shade area show 95% confidence intervals"}
SN_forecast_EFI %>% 
  # filter(site_id == 'SUGG') %>%
  pivot_wider(names_from = 'parameter', values_from = 'prediction') %>%
  ggplot(.,aes(x=datetime)) + 
  geom_ribbon(aes(ymax = mu + (1.96*sigma),
                  ymin = mu - (1.96*sigma)), alpha = 0.2, fill = 'blue') +
  geom_line(aes(y = mu)) + 
  facet_grid(variable~site_id, scales = 'free') + 
  theme_bw() +
  coord_cartesian(xlim = c(Sys.Date() - 10,
                           Sys.Date() + 30)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed') +
  geom_point(data = targets, aes(x=datetime, y= observation))
```
You can see that there are gaps in the forecasts. This is where there was no observation for that day in the previous year. Ways we might fill these gaps in the forecast include interpolating the values, taking a day-of-year mean for all years data. 

## Write the forecast for NEON EFI challenge
```{r eval = T}
# Start by writing the forecast to file
theme <- 'aquatics'
date <- Sys.Date()
forecast_name_2 <- 'GLEON_SN_forecast.csv'
forecast_file_2 <- paste(theme, date, forecast_name_2, sep = '-')

write_csv(SN_forecast_EFI, forecast_file_2)

```
## Submit forecast 
Files need to be in the correct format with metadata for submission. The forecast organisers have created tools to help aid in the submission process. These tools can be downloaded from Github using `devtools::install_github(eco4cast/neon4cast)`.
These include functions for submitting, scoring and reading forecasts:

* `submit()` - submit the forecast file to the neon4cast server where it will be scored
* `forecast_output_validator()` - will check the file is in the correct format to be submitted
* `check_submission()` - check that your submission has been uploaded to the server
* `score`

```{r eval = FALSE}
# can uses the neon4cast::forecast_output_validator() to check the forecast is in the right format
neon4cast::submit(forecast_file = forecast_file_2,
                  ask = F) # if ask = T (default), it will produce a pop-up box asking if you want to submit
```

## TASKS
Possible modifications to the Model 2 (lines x-x):

* fill gaps in forecasts - how could we modify the targets/forecast to get a continuous forecast for all sites
* explore Seasonal Naive and other simple models in [fable](https://otexts.com/fpp3/simple-methods.html)
* can we change the lag in the model
* another type of climatology model could include data from all years of historic data (easier outside fable)






# Introducing co-variates

One important step to overcome when thinking about generating forecasts is to introduce co-variates to the model. A water temperature forecast, for example, may be improved by additional information about past and future weather. The neon4cast challenge package includes functions for downloading past and future NOAA weather forecasts for all of the NEON sites. The 3 types of data are as follows:

* stage_1: raw forecasts - 31 member ensemble forecasts at 3 hr intervals for the first 10 days, and 6 hr intervals for up to 35 days at the NEON sites.
* stage_2: a processed version of Stage 1 in which fluxes are standardized to per second rates, fluxes and states are interpolated to 1 hour intervals and variables are renamed to match conventions. We recommend this for obtaining future weather by using `neon4cast::noaa_stage2()`. Future weather forecasts include a 30-member ensemble of equally likely future weather conditions.
* stage_3: can be viewed as the "historical" weather and is combination of day 1 weather forecasts (i.e., when the forecasts are most accurate). You can download this “stacked” NOAA product using `neon4cast::noaa_stage3()`.

These functions create a connection to the dataset hosted on the eco4cast server. To download the data you have to tell the function to `collect()` it. These data set can be subsetted and filtered using `dplyr` functions prior to download to limit the memory usage.

You can read more about the NOAA forecasts available for the NEON sites [here:](https://projects.ecoforecast.org/neon4cast-docs/Shared-Forecast-Drivers.html)

## Download co-variates
### Download historic data

We will generate a water temperature forecast using `air_temperature` as a co-variate. 

```{r, message=FALSE}
# past stacked weather
df_past <- neon4cast::noaa_stage3()
noaa_past <- df_past |> 
  dplyr::filter(site_id %in% aquatic_sites$field_site_id,
                datetime >= ymd('2017-01-01'),
                variable == "air_temperature") |> 
  dplyr::collect()

noaa_past
```

This is a stacked ensemble forecast of the one day ahead forecasts. To get an estimate of the historic conditions we can take a mean of these ensembles. We will also need to convert the temperatures to Celsius from Kelvin.

```{r}
# aggregate the past to mean values
noaa_past_mean <- noaa_past |> 
  mutate(datetime = as_date(datetime)) |> 
  group_by(datetime, site_id) |> 
  summarize(air_temperature = mean(prediction, na.rm = TRUE), .groups = "drop") |> 
  # convert air temp to C
  mutate(air_temperature = air_temperature - 273.15)
```

We can then look at the future weather forecasts in the same way but using the `noaa_stage2()`. The forecast becomes available from NOAA at 5am UTC the following day, so we take the air temperature forecast from yesterday to make the water quality forecasts. Then we can use the ensembles to produce uncertainty in the water quality forecast by grouping the forecasts by site and ensemble when aggregating. The forecasts are generate multiple times per day, so we can subset to just one, the forecast generated at midnight (`cycle = 0`).

### Download future weather forecasts

```{r, message=FALSE}
# New forecast only available at 5am UTC the next day

forecast_date <- Sys.Date() 
noaa_date <- forecast_date - days(1)

df_future <- neon4cast::noaa_stage2()

noaa_future <- df_future |> 
  dplyr::filter(reference_datetime == noaa_date,
                datetime >= forecast_date,
                site_id %in% aquatic_sites$field_site_id,
                variable == "air_temperature") |> 
  dplyr::collect()

```

The forecasts are hourly and we are interested in using daily mean weather for water temperature forecast generation.

```{r warning=F}
noaa_future_daily <- noaa_future |> 
  mutate(datetime = as_date(datetime)) |> 
  # mean daily forecasts at each site per ensemble
  group_by(datetime, site_id, parameter) |> 
  summarize(air_temperature = mean(prediction)) |>
  # convert to Celsius
  mutate(air_temperature = air_temperature - 273.15) |> 
  select(datetime, site_id, air_temperature, parameter)

noaa_future_daily
```

We will fit a simple linear model between historic air temperature and the water temperature targets data. Using this model we can then use our future estimates of air temperature (all 30 ensembles) to estimate water temperature at each site. The ensemble weather forecast will therefore propagate uncertainty into the water temperature forecast and give an estimate of driving data uncertainty. 

We will start by joining the historic weather data with the targets to aid in fitting the linear model.

```{r}
targets_lm <- targets |> 
  filter(variable == 'temperature') |>
  pivot_wider(names_from = 'variable', values_from = 'observation') |> 
  left_join(noaa_past_mean, 
            by = c("datetime","site_id"))
targets_lm[1000:1010,]
```


To fit the linear model we use the base R `lm()` but there are also methods to fit linear (and non-linear) models in the `fable::` package. You can explore the [documentation](https://otexts.com/fpp3/regression.html) for more information on the `fable::TSLM()` function. We can fit a separate linear model for each site. For example, at Lake Suggs, this would look like:

```{r, eval = F}
example_site <- 'SUGG'

site_target <- targets_lm |> 
  filter(site_id == example_site)

noaa_future_site <- noaa_future_daily |> 
  filter(site_id == example_site)

#Fit linear model based on past data: water temperature = m * air temperature + b
fit <- lm(site_target$temperature ~ site_target$air_temperature)
    
# use linear regression to forecast water temperature for each ensemble member
forecasted_temperature <- fit$coefficients[1] + fit$coefficients[2] * noaa_future_site$air_temperature

```
We can loop through this for each site to create a site-wise forecast of water temperature based on a linear model and each forecasted air temperature. We can run this forecast for each site and then bind them together to submit as one forecast. We can add in an if else statement for sites where there is no data collect.

## Specify forecast model

```{r}
temp_lm_forecast <- NULL

for(i in 1:length(lake_sites$field_site_id)) {  
  example_site <- lake_sites$field_site_id[i]
  
  site_target <- targets_lm |>
    filter(site_id == example_site)

  noaa_future_site <- noaa_future_daily |> 
    filter(site_id == example_site)
  
  # some sites have no temperature records so need to be skipped over
  if(length(which(!is.na(site_target$air_temperature) & !is.na(site_target$temperature))) > 0){
    
    #Fit linear model based on past data: water temperature = m * air temperature + b
    fit <- lm(site_target$temperature ~ site_target$air_temperature)
    
    # use linear regression to forecast water temperature for each ensemble member
    forecasted_temperature <- fit$coefficients[1] + fit$coefficients[2] * noaa_future_site$air_temperature
    
    # put all the relavent information into a tibble that we can bind together
    temperature <- tibble(datetime = noaa_future_site$datetime,
                          site_id = example_site,
                          parameter = noaa_future_site$parameter,
                          prediction = forecasted_temperature,
                          variable = "temperature")
    
    temp_lm_forecast <- dplyr::bind_rows(temp_lm_forecast, temperature)
    message(example_site, ' temperature forecast run')
  } else {
    message('no forecast run at ', example_site)
  }
}
```
We now have 30 possible forecasts of water temperature at each site and each day. On this plot each line represents one of the possible forecasts and the range of forecasted water temperature is a simple quantification of the uncertainty in our forecast. 
```{r, echo = F}
temp_lm_forecast %>%
  # filter(site_id == 'SUGG' | site_id == 'TOOK') %>%
  ggplot(., aes(x=datetime, y=prediction, group = parameter)) +
  geom_line() +
  facet_wrap(~site_id)
```

We need to make sure the dataframe is in the correct format and then we can submit this to the challenge as well! Similar to the RW, this is an ensemble forecast (specified in the `family` column). 

```{r}
temp_lm_forecast_EFI <- temp_lm_forecast %>%
  mutate(model_id = 'GLEON_lm',
         reference_datetime = Sys.Date(),
         family = 'ensemble',
         parameter = as.character(parameter)) %>%
  select(model_id, datetime, reference_datetime, site_id, family, parameter, variable, prediction)
```

## Submit forecast

```{r eval = T}
# Start by writing the forecast to file
theme <- 'aquatics'
date <- Sys.Date()
forecast_name_3 <- 'GLEON_LM_forecast.csv'
forecast_file_3 <- paste(theme, date, forecast_name_3, sep = '-')

write_csv(temp_lm_forecast_EFI, forecast_file_3)

```

```{r eval = FALSE}
# can uses the neon4cast::forecast_output_validator() to check the forecast is in the right format
neon4cast::submit(forecast_file = forecast_file_3,
                  ask = F) # if ask = T (default), it will produce a pop-up box asking if you want to submit
```

Is the linear model a reasonable relationship between air temperature and water temperature? Would some non-linear relationship be better? What about using yesterday's air and water temperatures to predict tomorrow? Or including additional parameters? There's a lot of variability in water temperatures unexplained by air temperature alone.

```{r, echo=F, warning=F}
ggplot(targets_lm, aes(x=air_temperature, y= temperature, colour = site_id)) +
  geom_point(alpha = 0.5) +
  theme_bw()
```


## TASKS
Possible modifications to the model 3 (lines x-x):

* include additional co-variates in the linear model (remember to 'collect' and subset the right data)
* include a lag in the predictors
* specify a non-linear relationship
* try forecasting another variable (oxygen or chlorophyll) - could you use your water temperature to estimate dissolved oxygen concentration at the surface?




# Other things that might be useful

## How the forecasts are scored?
The Challenge implements methods from the scoringRules R package to calculate the Continuous Rank Probability Score (CRPS) via the `score4cast` package. This scores the optimum is the minimum value, so we are aiming for as small a value as possible. CRPS uses information about the distribution of the forecasts (sd) as well as the estimated mean to calculate the score. There is some balance between accuracy and precision. The forecasts will also be compared with 'null' models (RW and climatology) More info in the [documentation](https://projects.ecoforecast.org/neon4cast-docs/Evaluation.html) or the `score4cast` package from EFI organizers [here](https://github.com/eco4cast/score4cast). 

## Other useful packages
Check out the NEON4cast R package ([introduction](https://projects.ecoforecast.org/neon4cast-docs/Helpful-Functions.html), and [github](https://github.com/eco4cast/neon4cast)) for other helpful functions when developing your workflow for the submitting to the challenge. 

EFI has also produced a package that summarizes the proposed community standards for the common formatting and archiving of ecological forecasts. Such open standards are intended to promote interoperability and facilitate forecast adoption, distribution, validation, and synthesis ([introduction](https://projects.ecoforecast.org/neon4cast-docs/Helpful-Functions.html#efistandards-package) and [github](https://github.com/eco4cast/EFIstandards))

## Alternative methods to loop through each variable-site_id combination
Using the `purrr` package we can also loop through each combination of site_id and variable combination. 
This is more efficient computationally than the for loop. You need to create a dataframe with each argument as a column. Then specify this, along with the RW function as arguments in `pmap_dfr`. The `dfr` part of the function specifies that the output should be use row_bind into a dataframe. 

```{r message = F, eval = F}
site_var_combinations <- 
  # Gets every combination of site_id and variable
  expand.grid(site = unique(targets$site_id),
              var = unique(targets$variable)) %>%
  # assign the transformation depending on the variable.
  mutate(transformation = 'none') %>%
  mutate(boot_number = 200,
         h = 30,
         bootstrap = T, 
         verbose = T)

# Runs the RW forecast for each combination of variable and site_id
RW_forecasts <- purrr::pmap_dfr(site_var_combinations, RW_daily_forecast) 

```

## Other weather variables 
You can look at what other variables are available in the NOAA weather data. There's information in the [Challenge documentation](https://projects.ecoforecast.org/neon4cast-docs/Shared-Forecast-Drivers.html) too.

```{r}
df_past %>%
  filter(site_id == 'ARIK',
         datetime > ymd('2022-01-01')) |> 
  dplyr::collect() |> 
  distinct(variable)
```

